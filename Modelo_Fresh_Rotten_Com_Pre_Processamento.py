import os
import shutil
import uuid
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import cv2
import kagglehub
from concurrent.futures import ThreadPoolExecutor, as_completed
from sklearn.decomposition import PCA
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
import time
import seaborn as sns
import hashlib

# ===============================================================================
# RODAR NO GOOGLE COLAB
# https://colab.research.google.com/drive/1qGWvekc4GR4pXq_vz3xFTGpCMO8AU3Rj?usp=sharing
# ===============================================================================

# ===============================================================================
# 0. PACOTES OBRIGATÃ“RIOS
#   pip install numpy
#   pip install matplotlib
#   pip install scikit-learn
#   pip install opencv-python
#   pip install seaborn
#   pip install kagglehub
#   
#   pip install numpy matplotlib scikit-learn opencv-python seaborn kagglehub
# 
#   Python 3.13.2:
#       python --version
# ===============================================================================

# ============================================================== 
# 1. BAIXAR DATASET
# ==============================================================

print("ğŸ“¥ Baixando dataset via kagglehub...")

dataset_path = kagglehub.dataset_download(
    "narayanibokde/augmented-dataset-for-fruits-rottenfresh"
)

print("ğŸ“¦ Dataset baixado em:", dataset_path)


# -----------------------------
# Limpeza automÃ¡tica de versÃµes antigas do kagglehub
# -----------------------------
def _find_kagglehub_version_parent(path):
    cur = os.path.dirname(path)
    # sobe atÃ© raiz, procurando pasta que contenha entradas 'v*'
    while True:
        try:
            entries = os.listdir(cur)
        except Exception:
            return None
        versions = [e for e in entries if e.startswith("v")]
        if len(versions) > 1:
            return cur
        parent = os.path.dirname(cur)
        if parent == cur:
            return None
        cur = parent

def limpar_kagglehub_versions(path_dataset):
    base_dir = _find_kagglehub_version_parent(path_dataset)
    if base_dir is None:
        print("Nenhuma pasta de versÃµes do KaggleHub encontrada para limpeza.")
        return

    versÃµes = sorted(
        [v for v in os.listdir(base_dir) if v.startswith("v")],
        reverse=True
    )

    if len(versÃµes) <= 1:
        print("Nenhuma versÃ£o extra do KaggleHub para limpar.")
        return

    print("\nğŸ§¹ Limpando versÃµes antigas do KaggleHub...\n")

    # MantÃ©m sÃ³ a primeira (mais recente)
    versÃµes_para_apagar = versÃµes[1:]

    for v in versÃµes_para_apagar:
        caminho = os.path.join(base_dir, v)
        try:
            shutil.rmtree(caminho, ignore_errors=True)
            print(f"   ğŸ”¥ Removido: {caminho}")
        except Exception as e:
            print(f"   âš  Falha ao remover {caminho}: {e}")

    print("\nâœ” VersÃµes antigas removidas com sucesso!\n")


# chama limpeza automÃ¡tica
try:
    limpar_kagglehub_versions(dataset_path)
except Exception as e:
    print("âš  Erro ao tentar limpar versÃµes do KaggleHub:", e)


# ============================================================== 
# 2. ENCONTRAR PASTAS ORIGINAIS
# ==============================================================

def localizar_pastas_brutas(path):
    candidatos = []
    for root, dirs, files in os.walk(path):
        for d in dirs:
            nome = d.lower()
            if nome.startswith("fresh") or nome.startswith("rotten"):
                candidatos.append(os.path.join(root, d))
    return candidatos

originais = localizar_pastas_brutas(dataset_path)

print("\nğŸ“‚ Pastas detectadas:")
for p in originais:
    print(" -", p)


# ============================================================== 
# 3. ORGANIZAR EM fresh/ rotten/ (com limpeza antes)
# ==============================================================

BASE_ORGANIZADA = os.path.join(dataset_path, "ORGANIZADO")
fresh_dir  = os.path.join(BASE_ORGANIZADA, "fresh")
rotten_dir = os.path.join(BASE_ORGANIZADA, "rotten")

# ğŸ”¥ LIMPEZA PARA EVITAR DUPLICAÃ‡ÃƒO
shutil.rmtree(BASE_ORGANIZADA, ignore_errors=True)
os.makedirs(fresh_dir, exist_ok=True)
os.makedirs(rotten_dir, exist_ok=True)

print("\nğŸ§¹ Limpando e reorganizando imagens...")

# -----------------------------
# FunÃ§Ãµes para copiar sem duplicar (baseadas em hash)
# -----------------------------
def file_md5(path, chunk_size=8192):
    h = hashlib.md5()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()

def build_hash_set(folder):
    hashes = set()
    if not os.path.exists(folder):
        return hashes
    for f in os.listdir(folder):
        p = os.path.join(folder, f)
        if os.path.isfile(p) and f.lower().endswith((".jpg",".jpeg",".png")):
            try:
                hashes.add(file_md5(p))
            except Exception:
                # ignora arquivos ilegÃ­veis
                pass
    return hashes

# inicializa conjuntos de hashes para evitar duplicaÃ§Ã£o
hashes_fresh = build_hash_set(fresh_dir)
hashes_rotten = build_hash_set(rotten_dir)

def copiar_sem_duplicar_por_hash(origem, destino_dir, hashes_set):
    try:
        md5 = file_md5(origem)
    except Exception:
        return False
    if md5 in hashes_set:
        return False
    novo_nome = f"{uuid.uuid4().hex}.jpg"
    destino_final = os.path.join(destino_dir, novo_nome)
    shutil.copy2(origem, destino_final)
    hashes_set.add(md5)
    return True

# agora mover_imagens usa cÃ³pia sem duplicaÃ§Ã£o por hash
total_copiadas_fresh = 0
total_copiadas_rotten = 0
copiados_por_origem = {}  # para log: {origem_dir: n_copiadas}

def mover_imagens(pasta):
    global total_copiadas_fresh, total_copiadas_rotten
    nome = os.path.basename(pasta).lower()
    if nome.startswith("fresh"):
        destino = fresh_dir
        hashes_set = hashes_fresh
    else:
        destino = rotten_dir
        hashes_set = hashes_rotten

    arquivos = [f for f in os.listdir(pasta) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
    copiadas = 0

    for f in arquivos:
        origem = os.path.join(pasta, f)
        if copiar_sem_duplicar_por_hash(origem, destino, hashes_set):
            copiadas += 1

    copiados_por_origem[pasta] = copiadas
    if nome.startswith("fresh"):
        total_copiadas_fresh += copiadas
    else:
        total_copiadas_rotten += copiadas

for pasta in originais:
    mover_imagens(pasta)

print("\nâœ” ReorganizaÃ§Ã£o concluÃ­da!")
print(f"â¡ fresh/ (novas cÃ³pias):  {total_copiadas_fresh} imagens")
print(f"â¡ rotten/ (novas cÃ³pias): {total_copiadas_rotten} imagens")

# Exibe detalhamento por pasta de origem
print("\nğŸ“‹ Detalhe de cÃ³pias por pasta de origem:")
for origem_pasta, qtd in copiados_por_origem.items():
    print(f" - {os.path.basename(origem_pasta)}: {qtd} imagens copiadas")


# ============================================================== 
# 4. CRIAR train/ val/ test (com limpeza antes)
# ==============================================================

base_out = "/content/dataset"

train_path = os.path.join(base_out, "train")
val_path   = os.path.join(base_out, "val")
test_path  = os.path.join(base_out, "test")

# ğŸ”¥ LIMPA TUDO ANTES DE CRIAR
shutil.rmtree(base_out, ignore_errors=True)

for folder in [train_path, val_path, test_path]:
    os.makedirs(os.path.join(folder, "fresh"), exist_ok=True)
    os.makedirs(os.path.join(folder, "rotten"), exist_ok=True)


def split_dataset(src, train_dir, val_dir, test_dir, split=(0.7, 0.15, 0.15)):
    files = [f for f in os.listdir(src) if f.lower().endswith(("jpg","png","jpeg"))]
    random.shuffle(files)

    total = len(files)
    n_train = int(split[0] * total)
    n_val   = int(split[1] * total)

    train_files = files[:n_train]
    val_files   = files[n_train:n_train+n_val]
    test_files  = files[n_train+n_val:]

    for f in train_files:
        shutil.copy2(os.path.join(src, f), os.path.join(train_dir, f))

    for f in val_files:
        shutil.copy2(os.path.join(src, f), os.path.join(val_dir, f))

    for f in test_files:
        shutil.copy2(os.path.join(src, f), os.path.join(test_dir, f))


split_dataset(fresh_dir,  os.path.join(train_path,"fresh"), 
                           os.path.join(val_path,"fresh"),
                           os.path.join(test_path,"fresh"))

split_dataset(rotten_dir, os.path.join(train_path,"rotten"), 
                           os.path.join(val_path,"rotten"),
                           os.path.join(test_path,"rotten"))


# ============================================================== 
# 5. CARREGAMENTO + PRÃ‰-PROCESSAMENTO
# ==============================================================

IMG_SIZE = (128, 128)

# PrÃ©-Processamento
def load_one_image(path_label):
    path, label = path_label
    img = cv2.imread(path)
    if img is None:
        return None
    img = cv2.resize(img, IMG_SIZE) # redimensionamento
    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # grayscale --> AlteraÃ§Ã£o para RGB
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # converte para RGB
    img = img.astype("float32") / 255.0 # normaliza
    return img.flatten(), label # flatten agora contÃ©m 3 canais

def carregar_pasta_threads(pasta, label):
    files = [
        (os.path.join(pasta, f), label)
        for f in os.listdir(pasta)
        if f.lower().endswith(("jpg","jpeg","png"))
    ]

    X, y = [], []

    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(load_one_image, fl) for fl in files]
        for future in as_completed(futures):
            result = future.result()
            if result is not None:
                img, lbl = result
                X.append(img)
                y.append(lbl)

    return np.array(X), np.array(y)


def carregar_dataset(base):
    xf, yf = carregar_pasta_threads(os.path.join(base, "fresh"), 0)
    xr, yr = carregar_pasta_threads(os.path.join(base, "rotten"), 1)
    X = np.concatenate([xf, xr])
    y = np.concatenate([yf, yr])
    return X, y


print("\nâš¡ Carregando imagens (multithreaded)...")

X_train, y_train = carregar_dataset(train_path)
X_val,   y_val   = carregar_dataset(val_path)
X_test,  y_test  = carregar_dataset(test_path)


# ============================================================== 
# 5.1 VISUALIZAÃ‡ÃƒO APÃ“S PRÃ‰-PROCESSAMENTO
# ==============================================================

print("\nğŸ” Exibindo amostras pÃ³s prÃ©-processamento...")

def show_examples(X, y):
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    fig.suptitle("Amostras pÃ³s prÃ©-processamento (128x128x3 RGB)")

    classes = [0, 1]  # fresh=0, rotten=1
    titles = ["Fresh", "Rotten"]

    for cls in classes:
        idx = np.where(y == cls)[0][:5]
        for i, img_idx in enumerate(idx):
            # img = X[img_idx].reshape(128, 128) # Para grayscale --> Mudamos para RGB
            img = X[img_idx].reshape(128, 128, 3) # Para RGB
            ax = axes[cls][i]
            ax.imshow(img, cmap="gray")
            ax.set_title(titles[cls])
            ax.axis("off")

    plt.show()

show_examples(X_train, y_train)

#========================================================================================================================================================================
# AlteraÃ§Ã£o de GRAYSCALE para RGB pois em termos de podridÃ£o em frutas, as manchas na casca podem variar muito de cor e tom, e usar GRAYSCALE fazia com que o modelo
# muitas vezes identificasse manchas mais leves como sombra / podridÃ£o, aumentando o nÃºmero de FN (Fresh, mas era Rotten) e FP (Rotten, mas era Fresh)
# Utilizar o RGB diminui um pouco a quantidade de FN e FP
#========================================================================================================================================================================


# ============================================================== 
# 6. NORMALIZAÃ‡ÃƒO + SVM
# ==============================================================

pipeline = Pipeline([
    ("pca", PCA(n_components=150, whiten=True, random_state=42)),
    ("svm", LinearSVC(C=1.0, max_iter=5000))
])

print("\nâ³ Treinando PCA + SVM...")
start_time = time.time()
pipeline.fit(X_train, y_train)
end_time = time.time()

print(f"\nâœ… Treinamento concluÃ­do em {end_time - start_time:.2f} segundos")

acc = pipeline.score(X_test, y_test)
print(f"ğŸ¯ AcurÃ¡cia: {acc:.4f}")


# ============================================================== 
# 7. MÃ‰TRICAS
# ==============================================================

# Precision â†’ quando o modelo diz fresh e realmente Ã© fresh
# Recall â†’ quanto ele encontra de todas as imagens realmente fresh
# F1-score â†’ equilÃ­brio entre precisÃ£o e recall

print("\nğŸ“ˆ Avaliando modelo...")

# LinearSVC nÃ£o tem predict_proba â†’ usar decision_function()
scores = pipeline.decision_function(X_test)
y_pred = (scores > 0).astype(int) # --> Bom equilÃ­brio
# y_pred = (scores > -0.2).astype(int) # --> Aumentando sensibilidade do modelo para detectar ROTTEN --> Descartado pois aumentou consideravelmente a taxa de desperdÃ­cio
# y_pred = (scores > -0.5).astype(int) # --> Aumentando sensibilidade do modelo para detectar ROTTEN --> Taxa de desperdÃ­cio e seguranÃ§a ALTAS
# y_pred = (scores > -1.0).astype(int) # --> Aumentando sensibilidade do modelo para detectar ROTTEN --> Vai priorizar SEGURANÃ‡A antes do DESPERDÃCIO Taxa de desperdÃ­cio e seguranÃ§a MUITO ALTAS

#========================================================================================================================================================================
# Cada aumento no treshold acima faz com que o modelo classifique mais Fresh como Rotten e menos Rotten como Fresh.
# Portanto quando menor o valor de corte, maior vai ser o desperdÃ­cio e menor vai ser o risco, por classificar mais Fresh como Rotten.
# E quanto maior for o valor de corte, menor vai ser o desperdÃ­cio e maior vai ser o risco, por classificar menos Fresh como Rotten.

# y_pred = (scores > 0).astype(int) # --> Visando o objetivo do projeto, esse treshold serÃ¡ priorizado, pois balanceia desperdÃ­cio e seguranÃ§a.
#========================================================================================================================================================================

print("\n===== CLASSIFICATION REPORT =====")
print(classification_report(y_test, y_pred, target_names=["Fresh", "Rotten"]))

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))
ax = sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")

# RÃ³tulos do eixo
ax.set_xlabel("Predito")
ax.set_ylabel("Real")
ax.set_title("Matriz de ConfusÃ£o com VP / FP / FN / VN")

# Substitui os rÃ³tulos numÃ©ricos pelos nomes das classes
ax.set_xticklabels(["Fresh", "Rotten"])
ax.set_yticklabels(["Fresh", "Rotten"])

# --- Adiciona texto explicando VP / FP / FN / VN em cada cÃ©lula ---
# cm = [[TN, FP],
#       [FN, TP]]

TN, FP = cm[0]
FN, TP = cm[1]

ax.text(0.5, 0.5, "VN\n(Verdadeiro Negativo)", ha="center", va="center", fontsize=10, color="black")
ax.text(1.5, 0.5, "FP\n(Falso Positivo)", ha="center", va="center", fontsize=10, color="black")
ax.text(0.5, 1.5, "FN\n(Falso Negativo)", ha="center", va="center", fontsize=10, color="black")
ax.text(1.5, 1.5, "VP\n(Verdadeiro Positivo)", ha="center", va="center", fontsize=10, color="black")

plt.show()

# VP â€“-> Verdadeiro Positivo â€“-> Predito Rotten e era Rotten
# FP â€“-> Falso Positivo	Predito â€“-> Rotten, mas era Fresh
# TN â€“-> Verdadeiro Negativo â€“-> Predito Fresh e era Fresh
# FN â€“-> Falso Negativo	Predito â€“-> Fresh, mas era Rotten

fpr, tpr, _ = roc_curve(y_test, scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0,1], [0,1], linestyle="--")
plt.title("Curva ROC")
plt.xlabel("Falso Positivo")
plt.ylabel("Verdadeiro Positivo")
plt.legend()
plt.show()

#=========================================================================================
# ROC:
#   Ela mostra como o modelo se comporta variando o limiar ("threshold") de decisÃ£o.
#       Se score > 0 â†’ predito Rotten (classe positiva)
#       Se score <= 0 â†’ predito Fresh
#   A curva ROC junta todos os possÃ­veis thresholds e plota:
#       Eixo X: FPR (Falso Positivo)
#       Eixo Y: TPR (Verdadeiro Positivo)

# "Rotten" Ã© a classe positiva:
# TPR alto â†’ modelo detecta frutas podres corretamente
# FPR baixo â†’ modelo quase nÃ£o marca fruitas boas como podres
# 90%+ de sensibilidade (boa detecÃ§Ã£o do podre)
# 9â€“10% de FP (marcar algumas frutas frescas como podres)

# Essa alta taxa AUC tem possÃ­veis causas:
#   Balanceamento muito bom entre as classes Rotten e Fresh
#   Dataset Ã© bem limpo --> com fundo uniforme em grande parte das imagens, 
#   muitas frutas isoladas, pouca sujeira visual, entre outros
#   ResoluÃ§Ã£o consistente das imagens do dataset
#=========================================================================================